<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="styles.css" />
    <title>Angel Lozada Servin - Cloud Resume</title>
</head>

<body>
    <nav class="top-nav">
        <button onclick="showSection('projects-section')">View Projects</button>
        <button onclick="showSection('resume-section')">View Resume</button>
    </nav>

    <div id="resume-section" style="display:none;">
        <h1>Angel Lozada Servin</h1>
        <p><strong>IT-focused Fulfillment Associate</strong> IT-focused Fulfillment Associate with 4+ years of
            experience in
            high-volume technical problem-solving and inventory logistics.
            Excel-certified with hands-on skills in SQL, cloud technologies (AWS/Azure).
            Actively transitioning into IT roles where analytical thinking, problem-solving, and technical skills drive
            business impact.</p>
        <div class="container">
            <div class="left-column">

                <h2>WORK EXPERIENCE</h2>
                <p><strong>Amazon, 800 N 75th Ave, PHX7, Phoenix, AZ, 85043 - <i>Fulfillment Associate & Technical
                            Problem Solver</i></strong>Sep 2021 – PRESENT</p>
                <ul class="responsibilities">
                    <li><strong>System Troubleshooting:</strong> Utilize Warehouse Management Systems (WMS) and handheld
                        scanning hardware to identify and resolve inventory discrepancies, issues, and system-level
                        bottlenecks.</li>
                    <li><strong>Data Integrity:</strong> Maintain 100% inventory accuracy by performing root cause
                        analysis on mis-shipped items.</li>
                    <li><strong>Efficiency & Performance:</strong> Resolve high-priority technical and logical issues
                        within 5-minute windows to ensure workflow continuity and hit delivery targets.</li>
                </ul>

                <h2>CERTIFICATIONS</h2>
                <div class="badge-container">
                    <img src="Images/Excel-badge.png" alt="Microsoft Office Specialist - Excel Associate Badge"
                        class="badge">
                    <ul class="badge-text">
                        <li><strong>Microsoft Office Specialist — Excel Associate (Microsoft 365 Apps)</strong></li>
                        <p>Issued on November 25, 2025.</p>
                    </ul>
                </div>

                <h2>TRAINING</h2>
                <div class="badge-container">
                    <img src="Images/AWS-badge.png" alt="AWS Academy Graduate - Cloud Foundations Badge" class="badge">

                    <ul class="badge-text">
                        <li><strong>AWS Academy Graduate - Cloud Foundations -</strong> <i>Training Badge</i></li>
                        <p>Issued on December 13, 2025.</p>
                    </ul>
                </div>
            </div>
            <div class="right-column">
                <h2>CONTACT</h2>
                <a href="mailto:anglozdaan@gmail.com">anglozdaan@gmail.com</a>

                (480) 561-7963

                Phoenix, AZ

                <a href="https://www.linkedin.com/in/angel-lozada-646a5037b">LinkedIn Profile</a>

                <h2>TECHNICAL SKILLS</h2>
                <ul class="skills">
                    <li><strong>Programming:</strong> SQL, Java, JavaScript, Python, HTML, CSS, PHP</li>
                    <li><strong>Tools:</strong> VS Code, MySQL Workbench, Microsoft Excel (pivot tables, charts,
                        array formulas, VLOOKUP/XLOOKUP, data migration), Git, GitHub, CI/CD pipelines, Bash/Command
                        Line</li>
                    <li><strong>Cloud services:</strong> AWS (IAM, S3, QuickSight), Azure (Power BI)</li>
                    <li><strong>Data:</strong> Data entry, migration, analysis, cleansing, visualization, durability
                        (versioning)</li>
                </ul>
                <h2>EDUCATION</h2>
                <p><strong>Estrella Mountain Community College</strong> - <i>AAS Information Technology</i></p>
                <p>May 2026 (Expected)</p>
                <p>Instruction in the principles of computer hardware components and business software, programming,
                    databases, networking, customer service, web development, information systems and project
                    management.</p>
            </div>
        </div>
    </div>

    <div id="projects-section">
        <h1>Technical Projects</h1>
        <div class="project-grid" onclick="toggleProject('s3-details')">
            <div class="project-header">
                <span class="project-title">Static Website Hosting</span>
                <img src="Images/HTML_logo.png" alt="HTML Logo" class="project-logo">
                <img src="Images/css_logo.png" alt="CSS Logo" class="project-logo">
                <img src="Images/Github_logo.png" alt="GitHub Logo" class="project-logo">
                <img src="Images/AWS_logo.png" alt="AWS Logo" class="project-logo">
                <span class="expand-icon">+</span>
            </div>

            <div id="s3-details" class="project-details" style="display:none;">
                <hr>
                <p><strong>Purpose:</strong> Showcase an understanding and proficiency in web hosting languages (HTML,
                    CSS, JavaScript) and deployment automation. The goal
                    wasn't just to build a static website but to automate its deployment process minimizing human error.
                </p>
                <ul class="details-list">
                    <li><strong>AWS S3:</strong> Configured an Amazon S3 bucket for static hosting. Altered the default
                        private bucket settings for public read access using a custom JSON bucket policy.
                    </li>
                    <img src="Images/static1.png" alt="Static Website Hosting AWS S3 Bucket" class="resource-image">
                    <img src="Images/static2.png" alt="Blocked Access to Static Website" class="resource-image">
                    <li><strong>Configured JSON Bucket Policy:</strong> Set up a bucket policy to allow public read
                        access to static website content. This is necessary to make the website publicly accessible.
                    </li>
                    <img src="Images/static3.png" alt="JSON Bucket Policy" class="resource-image">
                    <li><strong>GitHub Actions:</strong> Developed a YAML workflow to automate deployments to an AWS S3
                        bucket on every git
                        push to the main branch.</li>
                    <img src="Images/S3_JSON.png" alt="GitHub Actions Workflow for S3 Deployment"
                        class="resource-image">
                    <p><strong>Workflow Trigger & Setup:</strong> The workflow is named, "Deploy Resume to S3" and when
                        a push event occurs onto the main branch, it runs on an Ubuntu-latest runner.
                        This runner provides a fresh Linux environment in order to execute the deployment process.</p>

                    <p><strong>Code Checkout:</strong> The first step in the workflow uses the "actions/checkout@v3"
                        action to clone the latest repository code into the runner's workspace.
                        This step ensures that the runner has access to the most up-to-date version of the code and
                        project files are available for proceeding deployment steps.</p>

                    <p><strong>AWS Authentication:</strong> The second step in the workflow uses the
                        "aws-actions/configure-aws-credentials@v2" action to securely authenticate with AWS using
                        secrets stored in my GitHub repository (AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY).
                        The region is set to us-west-1, the region in which my AWS resources are configured and API
                        called are redirected to.</p>
                    <img src="Images/Secrets.png" alt="GitHub Secrets for AWS Credentials" class="resource-image">
                    <img src="Images/region.png" alt="AWS S3 Bucket Overview" class="resource-image">

                    <p><strong>S3 Deployment:</strong> The final step of the workflow uses the AWS CLI's "aws s3 sync"
                        command to synchronize the local project files with the S3 bucket named
                        "angel-cloud-resume-2026".
                        The "--delete" flag is included to remove any files in the S3 bucket that are no longer present
                        in the local repository, ensuring removed repository files are also removed from the S3 bucket.
                        This achieves accurate reflection of the current state of the project.
                        The "--exclude" flag is used to exclude any hidden files (starting with a dot, .github* and
                        .git*) from being uploaded to S3 bucket.</p>

                    <p><strong>Security Considerations:</strong> We must never store sensitive credentials in
                        version-controlled repositories or expose them in public code.
                        Instead, these keys are stored in GitHub's encrypted secrets manager which also happens to be
                        where this workflow retrieves them.</p>

                    <li><strong>Git CLI:</strong> Implemented Git to manage the project lifecycle, utilizing a
                        local-to-remote workflow to track changes in HTML, CSS, and associated files.</li>
                    <p class="descriptions">Repository Organization: Structure of the GitHub repository is organized in
                        a way that allows for easy navigation and management of project files.</p>
                    <img src="Images/Git.png" alt="GitHub Repository Structure" class="resource-image">

                    <li><strong>AWS IAM:</strong> Created IAM user "github-resume-deployer" with appropriate permissions
                        following the Principle of Least Privilege. The user is restricted from having console access
                        and is allowed only programmatic-only permissions for S3.</li>
                    <img src="Images/IAM1.png" alt="IAM User Policies" class="resource-image">
                    <p>Above is the brief summary of user "github-resume-deployer". The summary lists the Amazon
                        Resource Name (ARN) of the IAM user. It also displays console access as "Disabled", as this user
                        is only intended for programmatic access via GitHub Actions. This user is also assigned an
                        access key.</p>

                    <p>Below, we can see the policy name, "AmazonS3FullAccess". These policies are explicitly attached
                        to this user, hence the attached via "Directly" section. Note
                        this user has AmazonS3FullAccess permissions.</p>
                    <img src="Images/IAM_perm.png" alt="IAM User Permissions" class="resource-image">

                    <p>The following image displays an expanded view of the policy permissions. Here, the user has full
                        access to S3 and S3 Object Lambda operations.</p>
                    <img src="Images/IAM_perm_exp.png" alt="IAM User Permissions Expanded" class="resource-image">
                    <p class="descriptions" alt="IAM User JSON Policy Description">The JSON policy document defines the
                        permissions
                        granted to the IAM user "github-resume-deployer". It allows all actions ("s3:*") on all S3
                        resources ("arn:aws:s3:::*"), effectively granting full access to S3 services.</p>
                    <img src="Images/IAM_JSON.png" alt="IAM User JSON Policy" class="resource-image">
                </ul>
            </div>
        </div>


        <div class="project-grid" onclick="toggleProject('excel-details')">
            <div class="project-header">
                <span class="project-title">Fleet Analysis</span>
                <img src="Images/excel_logo.png" alt="Excel Logo" class="project-logo">
                <span class="expand-icon">+</span>
            </div>

            <div id="excel-details" class="project-details" style="display:none;">
                <hr>
                <p><strong>Purpose:</strong> Demonstrate advanced use of Excel functions, formulas and tools. Showcased
                    skills in data analysis, pivot tables, automation, and visualization. The goal was to answer
                    questions related to data interpretation and analysis of varied criteria.</p>
                <ul class="details-list">
                    <li><strong>Which truck manufacturer accrues the most maintenenace costs?</strong>
                        <a href="data/Logistics_Analysis.xlsx" class="download-button">
                            Download Full Analysis
                        </a>
                    </li>
                    <img src="Images/LA1.png" alt="Screenshot of excel pivot tables" class="resource-image">
                    <img src="Images/LA1_expanded.png" alt="Screenshot of VLOOKUP formula for cell M2"
                        class="resource-image">
                    <p><strong>VLOOKUP Function:</strong> In order to answer the question, VLOOKUP was used to match
                        truck IDs in the maintenance_records worksheet with the corresponding manufacturer information
                        in the trucks worksheet. This allows for the creation of pivot tables that display maintenance
                        costs by manufacturer.</p>

                    <p><strong>Pivot Tables:</strong> Pivot tables were created to summarize and analyze the maintenance
                        costs by manufacturer. The pivot table allowed for easy comparison of maintenance expenses
                        across different truck manufacturers.</p>
                    <p><strong>Slicers:</strong> Slicers were added to the pivot table to allow filtering of data by
                        different criteria. This provides for a more in depth
                        analysis of the data.</p>
                    <img src="Images/LA2.png" alt="Screenshot of excel pivot tables with slicers"
                        class="resource-image">
                    <p><i>The image above displays further analysis. It uses the slicers to filter the pivot table data
                            by maintenance_type and facility_location. Here, data displayed is that of costs accrued by
                            engine maintenance in Atlanta, Georgia.</i></p>
                    <li><strong>Realized Profits:</strong> In this worksheet, we use a multitude of functions to
                        summarize total accrued costs of each manufacturer and subtract them from the total revenue to
                        calculate realized profits.</li>
                    <img src="Images/realized_profits.png" alt="Overview of the realized_profits worksheet"
                        class="resource-image">
                    <p><strong>Unique:</strong> Use of the UNIQUE function to extract distinct values from a list of
                        manufacturers.</p>
                    <img src="Images/UNIQUE.png"
                        alt="Screenshot of UNIQUE function used to extract distinct manufacturers"
                        class="resource-image">
                    <p><strong>COUNTIF:</strong> Use of the COUNTIF function to calculate total number of trucks for
                        each manufacturer.</p>
                    <img src="Images/COUNTIF.png"
                        alt="Screenshot of COUNTIF function used to count total number of trucks for each manufacturer"
                        class="resource-image">
                    <p><strong>GETPIVOTDATA:</strong> Use of the GETPIVOTDATA function to extract specific data from a
                        pivot table. In this case, it is the total maintenance cost of each manufacturer.</p>
                    <img src="Images/GETPIVOTDATA.png"
                        alt="Screenshot of GETPIVOTDATA function used to extract total maintenance costs for each manufacturer"
                        class="resource-image">
                    <p><strong>Equation for Realized Profits:</strong> The realized profits for each manufacturer
                        are calculated by dividing realized profits by total revenue of each manufacturer.</p>
                    <img src="Images/Simple.png"
                        alt="Screenshot of equation used to calculate realized profits for each manufacturer"
                        class="resource-image">
                    <p><i>Note this column also has conditional formatting applied. Green fill is applied to
                            manifactures with an above average profit margin. Underperforming manufacturers are
                            highlighted in red fill.</i></p>
                </ul>
            </div>
        </div>

        <div class="project-grid" onclick="toggleProject('athena-details')">
            <div class="project-header">
                <span class="project-title">Serverless E-Commerce Data Lakehouse</span>
                <img src="Images/AWS_logo.png" alt="AWS Logo" class="project-logo">
                <span class="expand-icon">+</span>
            </div>

            <div id="athena-details" class="project-details" style="display:none;">
                <hr>
                <p><strong>Purpose:</strong> Engineer a scalable pipeline in order to transform e-commerce raw
                    data into business insights using AWS services.</p>
                <ul class="details-list">
                    <li><strong>Architecture:</strong> Designed a multi-tier S3 lake using Hive-style partioning. With
                        this partioning style, we can optimize query performance and storage costs by minimizing the
                        amount of data scanned during queries.</li>
                    <img src="Images/datalake_objects.png" alt="AWS S3 Lakehouse Architecture" class="resource-image">
                    <img src="Images/datalake_expanded.png" alt="AWS S3 Lakehouse Architecture Expanded"
                        class="resource-image">
                    <p><i>Note: The architecture is designed to be scalable and cost-effective. Instead of having to
                            scan through one file with multiple partitions, we can scan only the relevant partitions for
                            each query. Note the second image displays an expanded view of one of the partition
                            ("customers/olist_customers_dataset.csv")s.</i></p>
                    <li><strong>Data Orchestration:</strong> Created and deployed an AWS Glue Crawler to connect to the
                        S3 data lake and generate metadata tables for querying in AWS Athena.</li>
                    <img src="Images/crawler.png" alt="AWS Glue Crawler Configuration" class="resource-image">
                    <img src="Images/crawler_data_source.png" alt="AWS Glue Crawler Data Source Configuration"
                        class="resource-image">
                    <p><i> The data source of the crawler refers back to the newly created S3 bucket
                            "s3://angels-data-lake-project-2026" used as the "data lake". It sotres each of the .csv
                            files.</i></p>
                    <img src="Images/crawler_properties.png" alt="AWS Glue Crawler Properties Configuration"
                        class="resource-image">
                    <p><i>In the properties configuration above, we specify the crawler to use an existing IAM role with
                            the necessary permissions to access the S3 bucket and create metadata tables. We also
                            specify the crawler to update the "olist_db" database in AWS Glue Data Catalog with the
                            generated metadata tables.</i></p>
                    <p><strong>IAM Role:</strong> Specific permissions have been applied to the IAM role used by the
                        crawler. The allowed permission policies include AWSS3ReadOnlyAccess and AWSGlueServiceRole. An
                        expanded view of the IAM role permissions is shown in the image below.</p>
                    <img src="Images/IAM_role_permission_policies.png" alt="IAM Role Permission Policies"
                        class="resource-image">
                    <img src="Images/IAM_role_ReadOnly.png" alt="IAM Role Read Only Access to S3"
                        class="resource-image">
                    <p><i>The <b>"AWSReadOnlyAccess"</b> policy grants read-only access to S3 buckets and objects. This is
                            absolutely necessary for the crawler to be able to read the data in the S3 bucket and create
                            metadata tables to fill AWS Glue Data Catalog.</i></p>
                    <img src="Images/IAM_role_GlueServiceRole.png" alt="IAM Role AWS Glue Service Role Permissions"
                        class="resource-image">
                    <p><i>The <b>"AWSGlueServiceRole"</b> policy grants permissions for the crawler to create and manage
                            metadata tables in AWS Glue Data Catalog. Everything from updating table definitions to
                            creating new tables is allowed. This is necessary for the crawler to be able to
                            create metadata tables based on the data in the S3 bucket and insert them into the
                            "olist_db"
                            database in AWS Glue Data Catalog.</i></p>
                    <p>In the following image, we see when running this crawler, the creation of 9 tables. These
                        tables represent automated creation of metadata for each of the .csv files in the data lake.</i>
                    </p>
                    <img src="Images/crawler_log_events.png" alt="AWS Glue Crawler Log Events" class="resource-image">
                    <p>After the crawler finishes running, we can query the metadata tables in AWS Athena. Thanks to the
                        Crawler, metadata tables have been created and insterted into "olist_db".</p>
                    <img src="Images/olist_db.png" alt="AWS Athena Database with Crawler Generated Metadata Tables"
                        class="resource-image">
                </ul>
            </div>
        </div>
    </div>

    <script>
        function showSection(sectionId) {
            document.getElementById('projects-section').style.display = 'none';
            document.getElementById('resume-section').style.display = 'none';
            document.getElementById(sectionId).style.display = 'block';
        }

        function toggleProject(detailId) {
            const details = document.getElementById(detailId);
            const icon = details.parentElement.querySelector('.expand-icon');

            if (details.style.display === "none") {
                details.style.display = "block";
                icon.innerText = "−";
            } else {
                details.style.display = "none";
                icon.innerText = "+";
            }
        }
    </script>
</body>

</html>